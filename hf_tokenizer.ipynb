{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE, Unigram, WordLevel, WordPiece\n",
    "from tokenizers.trainers import BpeTrainer, WordLevelTrainer, \\\n",
    "                                WordPieceTrainer, UnigramTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_token = \"<UNK>\"  # token for unknown words\n",
    "spl_tokens = [\"<UNK>\", \"<SEP>\", \"<MASK>\", \"<CLS>\"]  # special tokens\n",
    "\n",
    "def prepare_tokenizer_trainer(alg):\n",
    "    \"\"\"\n",
    "    Prepares the tokenizer and trainer with unknown & special tokens.\n",
    "    \"\"\"\n",
    "    if alg == 'BPE':\n",
    "        tokenizer = Tokenizer(BPE(unk_token = unk_token))\n",
    "        trainer = BpeTrainer(special_tokens = spl_tokens)\n",
    "    elif alg == 'UNI':\n",
    "        tokenizer = Tokenizer(Unigram())\n",
    "        trainer = UnigramTrainer(unk_token= unk_token, special_tokens = spl_tokens)\n",
    "    elif alg == 'WPC':\n",
    "        tokenizer = Tokenizer(WordPiece(unk_token = unk_token))\n",
    "        trainer = WordPieceTrainer(special_tokens = spl_tokens)\n",
    "    else:\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token = unk_token))\n",
    "        trainer = WordLevelTrainer(special_tokens = spl_tokens)\n",
    "    \n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    return tokenizer, trainer\n",
    "\n",
    "\n",
    "def train_tokenizer(files, alg='WLV'):\n",
    "    \"\"\"\n",
    "    Takes the files and trains the tokenizer.\n",
    "    \"\"\"\n",
    "    tokenizer, trainer = prepare_tokenizer_trainer(alg)\n",
    "    tokenizer.train(files, trainer) # training the tokenzier\n",
    "    tokenizer.save(\"./tokenizer-trained.json\")\n",
    "    tokenizer = Tokenizer.from_file(\"./tokenizer-trained.json\")\n",
    "    return tokenizer\n",
    "\n",
    "def tokenize(input_string, tokenizer):\n",
    "    \"\"\"\n",
    "    Tokenizes the input string using the tokenizer provided.\n",
    "    \"\"\"\n",
    "    output = tokenizer.encode(input_string)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========Using vocabulary from ['veri/alice.txt']=======\n",
      "---- WLV ----\n",
      "['<UNK>', '<UNK>', 'a', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '.', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', 'in', 'a', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '.', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '.', '<UNK>', '<UNK>', '<UNK>'] -> 35\n",
      "\n",
      "\n",
      "\n",
      "---- BPE ----\n",
      "['T', 'his', 'is', 'a', 'de', 'ep', 'le', 'ar', 'nin', 'g', 't', 'o', 'ken', 'iz', 'a', 'ti', 'on', 'tut', 'or', 'i', 'al', '.', 'To', 'ken', 'iz', 'a', 'ti', 'on', 'is', 't', 'he', 'f', 'ir', 'st', 'ste', 'p', 'in', 'a', 'de', 'ep', 'le', 'ar', 'nin', 'g', 'N', 'L', 'P', 'p', 'ip', 'eline', '.', 'W', 'e', 'w', 'il', 'l', 'be', 'c', 'om', 'par', 'ing', 't', 'he', 't', 'o', 'ken', 's', 'gen', 'er', 'a', 'te', 'd', 'b', 'y', 'e', 'ac', 'h', 't', 'o', 'ken', 'iz', 'a', 'ti', 'on', 'mo', 'de', 'l', '.', 'E', '<UNK>', 'ci', 'te', 'd', 'mu', 'c', 'h', '?!', '<UNK>'] -> 98\n",
      "\n",
      "\n",
      "---- UNI ----\n",
      "['T', 'h', 'is', 'is', 'a', 'de', 'e', 'p', 'le', 'ar', 'nin', 'g', 't', 'o', 'ken', 'i', 'zat', 'i', 'on', 'tut', 'or', 'i', 'al', '.', 'To', 'ken', 'i', 'zat', 'i', 'on', 'is', 'th', 'e', 'fi', 'r', 'st', 's', 'tep', 'in', 'a', 'de', 'e', 'p', 'le', 'ar', 'nin', 'g', 'N', 'L', 'P', 'p', 'ip', 'elin', 'e', '.', 'W', 'e', 'wi', 'l', 'l', 'be', 'c', 'o', 'm', 'p', 'ar', 'in', 'g', 'th', 'e', 't', 'o', 'ken', 's', 'gene', 'r', 'at', 'ed', 'b', 'y', 'e', 'a', 'c', 'h', 't', 'o', 'ken', 'i', 'zat', 'i', 'on', 'm', 'o', 'de', 'l', '.', 'E', 'x', 'ci', 'te', 'd', 'mu', 'c', 'h', '?!', 'üòç'] -> 106\n",
      "\n",
      "\n",
      "\n",
      "---- WPC ----\n",
      "['T', '##h', '##is', 'is', 'a', 'de', '##ep', 'le', '##ar', '##ni', '##n', '##g', 'tok', '##eni', '##za', '##ti', '##on', 'tut', '##or', '##i', '##al', '.', 'Tok', '##eni', '##za', '##ti', '##on', 'is', 't', '##he', 'fi', '##r', '##st', 's', '##te', '##p', 'in', 'a', 'de', '##ep', 'le', '##ar', '##ni', '##n', '##g', 'N', '##L', '##P', 'p', '##ip', '##el', '##ine', '.', 'W', '##e', 'w', '##il', '##l', 'be', 'c', '##o', '##m', '##pa', '##ri', '##n', '##g', 't', '##he', 'tok', '##en', '##s', 'gene', '##ra', '##te', '##d', 'b', '##y', 'e', '##a', '##c', '##h', 'tok', '##eni', '##za', '##ti', '##on', 'mo', '##de', '##l', '.', '<UNK>', 'mu', '##c', '##h', '<UNK>'] -> 95\n",
      "========Using vocabulary from ['veri/alice.txt', 'veri/alice.txt', 'veri/alice.txt']=======\n",
      "---- WLV ----\n",
      "['<UNK>', '<UNK>', 'a', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '.', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', 'in', 'a', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '.', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '.', '<UNK>', '<UNK>', '<UNK>'] -> 35\n",
      "\n",
      "\n",
      "\n",
      "---- BPE ----\n",
      "['T', 'his', 'is', 'a', 'de', 'ep', 'le', 'ar', 'nin', 'g', 't', 'o', 'ken', 'iz', 'a', 'ti', 'on', 'tut', 'or', 'i', 'al', '.', 'To', 'ken', 'iz', 'a', 'ti', 'on', 'is', 't', 'he', 'f', 'ir', 'st', 'ste', 'p', 'in', 'a', 'de', 'ep', 'le', 'ar', 'nin', 'g', 'N', 'L', 'P', 'p', 'ip', 'eline', '.', 'W', 'e', 'w', 'il', 'l', 'be', 'c', 'om', 'par', 'ing', 't', 'he', 't', 'o', 'ken', 's', 'gen', 'er', 'a', 'te', 'd', 'b', 'y', 'e', 'ac', 'h', 't', 'o', 'ken', 'iz', 'a', 'ti', 'on', 'mo', 'de', 'l', '.', 'E', '<UNK>', 'ci', 'te', 'd', 'mu', 'c', 'h', '?!', '<UNK>'] -> 98\n",
      "\n",
      "\n",
      "---- UNI ----\n",
      "['T', 'h', 'is', 'is', 'a', 'de', 'e', 'p', 'le', 'ar', 'nin', 'g', 't', 'o', 'ken', 'i', 'zat', 'i', 'on', 'tut', 'or', 'i', 'al', '.', 'To', 'ken', 'i', 'zat', 'i', 'on', 'is', 't', 'he', 'f', 'ir', 'st', 's', 'tep', 'in', 'a', 'de', 'e', 'p', 'le', 'ar', 'nin', 'g', 'N', 'L', 'P', 'p', 'ip', 'elin', 'e', '.', 'W', 'e', 'wi', 'l', 'l', 'b', 'e', 'c', 'o', 'm', 'p', 'ar', 'in', 'g', 't', 'he', 't', 'o', 'ken', 's', 'gene', 'r', 'at', 'e', 'd', 'b', 'y', 'e', 'a', 'c', 'h', 't', 'o', 'ken', 'i', 'zat', 'i', 'on', 'm', 'o', 'de', 'l', '.', 'E', 'x', 'ci', 'te', 'd', 'mu', 'c', 'h', '?!', 'üòç'] -> 108\n",
      "\n",
      "\n",
      "\n",
      "---- WPC ----\n",
      "['T', '##h', '##is', 'is', 'a', 'de', '##ep', 'le', '##ar', '##ni', '##n', '##g', 'tok', '##eni', '##za', '##ti', '##on', 'tut', '##or', '##i', '##al', '.', 'Tok', '##eni', '##za', '##ti', '##on', 'is', 't', '##he', 'fi', '##r', '##st', 's', '##te', '##p', 'in', 'a', 'de', '##ep', 'le', '##ar', '##ni', '##n', '##g', 'N', '##L', '##P', 'p', '##ip', '##el', '##ine', '.', 'W', '##e', 'w', '##il', '##l', 'be', 'c', '##o', '##m', '##pa', '##ri', '##n', '##g', 't', '##he', 'tok', '##en', '##s', 'gene', '##ra', '##te', '##d', 'b', '##y', 'e', '##a', '##c', '##h', 'tok', '##eni', '##za', '##ti', '##on', 'mo', '##de', '##l', '.', '<UNK>', 'mu', '##c', '##h', '<UNK>'] -> 95\n"
     ]
    }
   ],
   "source": [
    "##training on a small dataset\n",
    "small_file = ['veri/alice.txt']\n",
    "# large_files = [f\"./wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
    "large_files = ['veri/alice.txt', 'veri/alice.txt', 'veri/alice.txt']\n",
    "\n",
    "tokens_dict = {}\n",
    "\n",
    "for files in [small_file, large_files]:\n",
    "    print(f\"========Using vocabulary from {files}=======\")\n",
    "    for alg in ['WLV', 'BPE', 'UNI', 'WPC']:\n",
    "        trained_tokenizer = train_tokenizer(files, alg)\n",
    "        input_string = \"This is a deep learning tokenization tutorial. Tokenization is the first step in a deep learning NLP pipeline. We will be comparing the tokens generated by each tokenization model. Excited much?!üòç\"\n",
    "        output = tokenize(input_string, trained_tokenizer)\n",
    "        tokens_dict[alg] = output.tokens\n",
    "        print(\"----\", alg, \"----\")\n",
    "        print(output.tokens, \"->\", len(output.tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokens_dict = {}\n",
    "\n",
    "for alg in ['BPE', 'UNI', 'WPC']:\n",
    "    trained_tokenizer = train_tokenizer(large_files, alg)\n",
    "    input_string = \"This is a deep learning tokenization tutorial. Tokenization is the first step in a deep learning NLP pipeline. We will be comparing the tokens generated by each tokenization model. Excited much?!üòç\"\n",
    "    output = tokenize(input_string, trained_tokenizer)\n",
    "    tokens_dict[alg] = output.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "max_len = max(len(tokens_dict['UNI']), len(tokens_dict['WPC']), len(tokens_dict['BPE']))\n",
    "diff_bpe = max_len - len(tokens_dict['BPE'])\n",
    "diff_wpc = max_len - len(tokens_dict['WPC'])\n",
    "\n",
    "tokens_dict['BPE'] = tokens_dict['BPE'] + ['<PAD>']*diff_bpe\n",
    "tokens_dict['WPC'] = tokens_dict['WPC'] + ['<PAD>']*diff_wpc\n",
    "\n",
    "\n",
    "df = pd.DataFrame(tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BPE</th>\n",
       "      <th>UNI</th>\n",
       "      <th>WPC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>his</td>\n",
       "      <td>h</td>\n",
       "      <td>##h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is</td>\n",
       "      <td>is</td>\n",
       "      <td>##is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>is</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>de</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ep</td>\n",
       "      <td>de</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>le</td>\n",
       "      <td>e</td>\n",
       "      <td>##ep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ar</td>\n",
       "      <td>p</td>\n",
       "      <td>le</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nin</td>\n",
       "      <td>le</td>\n",
       "      <td>##ar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>g</td>\n",
       "      <td>ar</td>\n",
       "      <td>##ni</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   BPE UNI   WPC\n",
       "0    T   T     T\n",
       "1  his   h   ##h\n",
       "2   is  is  ##is\n",
       "3    a  is    is\n",
       "4   de   a     a\n",
       "5   ep  de    de\n",
       "6   le   e  ##ep\n",
       "7   ar   p    le\n",
       "8  nin  le  ##ar\n",
       "9    g  ar  ##ni"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BPE</th>\n",
       "      <th>UNI</th>\n",
       "      <th>WPC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>108</td>\n",
       "      <td>108</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>60</td>\n",
       "      <td>52</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>&lt;PAD&gt;</td>\n",
       "      <td>e</td>\n",
       "      <td>&lt;PAD&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          BPE  UNI    WPC\n",
       "count     108  108    108\n",
       "unique     60   52     65\n",
       "top     <PAD>    e  <PAD>\n",
       "freq       10    7     13"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include= 'all')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
